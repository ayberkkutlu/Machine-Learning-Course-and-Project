{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b421783",
   "metadata": {},
   "source": [
    "## Item-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ec201",
   "metadata": {},
   "source": [
    "### Apply and Evaluate Model to Test Set\n",
    "\n",
    "#### Objective:\n",
    "Run predictions for your test set and evaluate logistic regression model accuracy\n",
    "#### Steps:\n",
    "* Switch feature matrix to test set\n",
    "* Predict\n",
    "* Compute percentage of agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebf4e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f468b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    233\n",
       "0    204\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\ayber\\OneDrive\\Masaüstü\\ML\\Data\\external\\Fatigue_data.csv\").copy()\n",
    "target='Fatigue'\n",
    "\n",
    "X = df.drop(columns=['Sl. No.', target]).copy()\n",
    "y_reg = df[target].to_numpy()\n",
    "y = (y_reg > 500).astype(int)     # 1=good, 0=bad\n",
    "\n",
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89a773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60/20/20 with stratification (important for class balance)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e487e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --Helpers for future use--\n",
    "def fit_and_print_clf(p, model_name, thr=0.5, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    p.fit(X_train, y_train)\n",
    "\n",
    "    # feature count before the estimator\n",
    "    Z_train = p[:-1].transform(X_train)\n",
    "    n_features = Z_train.shape[1]\n",
    "\n",
    "    # probabilities -> labels at threshold\n",
    "    proba_tr = p.predict_proba(X_train)[:, 1]\n",
    "    proba_te = p.predict_proba(X_test)[:, 1]\n",
    "    yhat_tr  = (proba_tr >= thr).astype(int)\n",
    "    yhat_te  = (proba_te >= thr).astype(int)\n",
    "\n",
    "    # metrics\n",
    "    acc_tr = accuracy_score(y_train, yhat_tr) * 100.0\n",
    "    acc_te = accuracy_score(y_test,  yhat_te) * 100.0\n",
    "    auc_tr = roc_auc_score(y_train, proba_tr)\n",
    "    auc_te = roc_auc_score(y_test,  proba_te)\n",
    "\n",
    "    print(\"Model Name ->\", model_name)\n",
    "    print(\"Feature count:\", n_features)\n",
    "    print(f\"Agreement (train): {acc_tr:.2f}% | AUC: {auc_tr:.3f}\")\n",
    "    print(f\"Agreement (test):  {acc_te:.2f}% | AUC: {auc_te:.3f}\")\n",
    "    print(\"Confusion matrix (test):\\n\", confusion_matrix(y_test, yhat_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4daccbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name -> Logistic Regression\n",
      "Feature count: 350\n",
      "Agreement (train): 100.00% | AUC: 1.000\n",
      "Agreement (test):  89.77% | AUC: 0.979\n",
      "Confusion matrix (test):\n",
      " [[36  5]\n",
      " [ 4 43]]\n",
      "Best C: 1.2115276586285888\n"
     ]
    }
   ],
   "source": [
    "p1= Pipeline([\n",
    "    (\"poly\",  PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"clf\",   LogisticRegressionCV(\n",
    "        Cs=np.logspace(-2, 3, 13),\n",
    "        cv=5,\n",
    "        max_iter=10000,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        refit=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "fit_and_print_clf(p1, model_name=\"Logistic Regression\")\n",
    "print(\"Best C:\", p1.named_steps[\"clf\"].C_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd206c",
   "metadata": {},
   "source": [
    "## Item-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99ddde",
   "metadata": {},
   "source": [
    "### Improve Model Accuracy\n",
    "\n",
    "#### Objective:\n",
    "Try to improve test set agreement percentage\n",
    "#### Steps:\n",
    " * Create/select/exclude features\n",
    " * Train + Perform logistic regression\n",
    " * Compute percentage of agreement and find best one you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f49f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name -> Logistic Regression\n",
      "Feature count: 25\n",
      "Agreement (train): 96.93% | AUC: 0.996\n",
      "Agreement (test):  93.18% | AUC: 0.972\n",
      "Confusion matrix (test):\n",
      " [[38  3]\n",
      " [ 3 44]]\n",
      "Best C: 2.5118864315095824\n"
     ]
    }
   ],
   "source": [
    "#Feature exclusion and parameter tuning done on the previous model\n",
    "p2= Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"clf\",   LogisticRegressionCV(\n",
    "        Cs=np.logspace(-2, 3, 26),\n",
    "        cv=5,\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=10000,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        refit=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "fit_and_print_clf(p2, model_name=\"Logistic Regression with \")\n",
    "print(\"Best C:\", p2.named_steps[\"clf\"].C_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e672770",
   "metadata": {},
   "source": [
    "## Item-13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f5384",
   "metadata": {},
   "source": [
    "### Use RR on your dataset\n",
    "\n",
    "#### Objective:\n",
    "Compare Ridge Regression result to your previous models\n",
    "#### Steps:\n",
    " * Create/Select features similar to before\n",
    " * Perform ridge regression on these using scikit-learn\n",
    " * Compare to the weights you get in your regular regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa7fe28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea1e5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Sl. No.', target]).copy()\n",
    "y = df['Fatigue'].to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train, X_val,  y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25dc3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "def fit_and_print(p, model_name, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, X_val=X_val, y_val=y_val):\n",
    "    p.fit(X_train,y_train)\n",
    "    train_preds= p.predict(X_train)\n",
    "    validation_preds = p.predict(X_val)\n",
    "    test_preds= p.predict(X_test)\n",
    "    \n",
    "    \n",
    "    Z_train = p[:-1].transform(X_train)\n",
    "    n_features = Z_train.shape[1]\n",
    "    print(\"Model Name -> \" + str(model_name))\n",
    "    print(\"Feature count \" + str(n_features))\n",
    "    print(\"Training error: \" + str(rmse(train_preds, y_train)))\n",
    "    print(\"Validation error: \" + str(rmse(validation_preds, y_val)))\n",
    "    print(\"Testing error: \" + str(rmse(test_preds, y_test)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69b95935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name -> Ridge Regression\n",
      "Feature count 350\n",
      "Training error: 17.612567878994412\n",
      "Validation error: 26.777701024099567\n",
      "Testing error: 30.5408097784356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p3= Pipeline([\n",
    "    (\"poly\",  PolynomialFeatures(degree=2, include_bias=False)), \n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"ridge\", RidgeCV(alphas=np.logspace(-4, 4, 33), cv=5))\n",
    "])\n",
    "\n",
    "fit_and_print(p3, model_name=\"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e82ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name -> Regular Linear Regression\n",
      "Feature count 350\n",
      "Training error: 6.354231662215995\n",
      "Validation error: 213.37353878450864\n",
      "Testing error: 389.6138200719838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p4= Pipeline([\n",
    "    (\"poly\",  PolynomialFeatures(degree=2, include_bias=False)), \n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"ols\",   LinearRegression())\n",
    "])\n",
    "\n",
    "fit_and_print(p4, model_name=\"Regular Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57413e0",
   "metadata": {},
   "source": [
    "## Item-14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24960e4d",
   "metadata": {},
   "source": [
    "### Use KRR on your dataset\n",
    "\n",
    "#### Objective:\n",
    "* Compare Kernel Ridge Regression result to your previous models.\n",
    "\n",
    "#### Steps:\n",
    "* Create/Select features similar to before\n",
    "* Perform kernel ridge regression on these using sciki-learn\n",
    "* Test if your model is overfit or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a267ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b5bcecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR-RBF best (VAL): {'alpha': 0.01, 'gamma': 0.01} | val RMSE: 22.3610\n"
     ]
    }
   ],
   "source": [
    "def fit_krr_rbf_by_val(X_train, y_train, X_val, y_val,\n",
    "                       alphas=(1e-3, 1e-2, 1e-1, 1, 10),\n",
    "                       gammas=(1e-3, 1e-2, 1e-1, 1.0)):\n",
    "    best_pipe, best_rmse, best_params = None, float(\"inf\"), None\n",
    "    for a, g in product(alphas, gammas):\n",
    "        pipe = Pipeline([\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"krr\", KernelRidge(kernel=\"rbf\", alpha=a, gamma=g))\n",
    "        ])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        val_rmse = rmse(y_val, pipe.predict(X_val))\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_pipe = pipe\n",
    "            best_params = {\"alpha\": a, \"gamma\": g}\n",
    "    return best_pipe, best_params, best_rmse\n",
    "\n",
    "best_krr, best_params, val_rmse = fit_krr_rbf_by_val(X_train, y_train, X_val, y_val)\n",
    "print(\"KRR-RBF best (VAL):\", best_params, \"| val RMSE:\", f\"{val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30b2cd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR-RBF RMSE  train+val= 14.4204  test= 21.6197\n"
     ]
    }
   ],
   "source": [
    "X_tv = pd.concat([X_train, X_val], axis=0)\n",
    "y_tv = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "# rebuild with the same hyperparameters and fit on train+val\n",
    "p5 = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"krr\", KernelRidge(kernel=\"rbf\", **best_params))\n",
    "]).fit(X_tv, y_tv)\n",
    "\n",
    "print(\"KRR-RBF RMSE  train+val=\", f\"{rmse(y_tv,  p5.predict(X_tv)):.4f}\",\n",
    "      \" test=\", f\"{rmse(y_test, p5.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8af7958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def overfit_report_regressor(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    ytr = model.predict(X_train)\n",
    "    yva = model.predict(X_val)\n",
    "    yte = model.predict(X_test)\n",
    "    tr, va, te = rmse(y_train, ytr), rmse(y_val, yva), rmse(y_test, yte)\n",
    "    gap_va = va - tr\n",
    "    gap_te = te - tr\n",
    "    print(f\"RMSE  train={tr:.4f}  val={va:.4f}  test={te:.4f}\")\n",
    "    print(f\"Generalization gap:  val-train={gap_va:.4f} | test-train={gap_te:.4f}\")\n",
    "    if gap_va > 0 and gap_te > 0 and (gap_va > 0.1*abs(va) or gap_te > 0.1*abs(te)):\n",
    "        print(\"⚠️ Likely overfitting: train error is much lower than val/test.\")\n",
    "    else:\n",
    "        print(\"✅ No strong overfitting signal based on gaps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE  train=6.3542  val=213.3735  test=389.6138\n",
      "Generalization gap:  val-train=207.0193 | test-train=383.2596\n",
      "⚠️ Likely overfitting: train error is much lower than val/test.\n"
     ]
    }
   ],
   "source": [
    "#I will test the funciton on my OLR model which I know overfitting\n",
    "overfit_report_regressor(p4, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE  train=14.5836  val=13.9252  test=21.6197\n",
      "Generalization gap:  val-train=-0.6584 | test-train=7.0361\n",
      "✅ No strong overfitting signal based on gaps.\n"
     ]
    }
   ],
   "source": [
    "#Now I will test if my KNN model overfitting strongly\n",
    "overfit_report_regressor(p5, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
